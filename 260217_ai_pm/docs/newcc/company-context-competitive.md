---
type: guide
status: revise
track: context
---

# Company Context - Competitive

**빠른 요약**
- Company Context - Competitive 주제를 빠르게 이해하고 바로 써볼 수 있게 정리한 문서입니다.

**이번 문서 목표**
- 읽고 나면 Company Context - Competitive 내용을 내 업무 흐름에 맞게 적용할 수 있습니다.

**처음 보는 분 가이드**
- 먼저 `## 1) 목적`과 `## 5) 실습/적용 체크리스트`만 읽고 시작하세요.
- 용어가 낯설면 [사전](./사전.md)을 먼저 확인하세요.

## 1) 목적
- 경쟁 분석을 기능 나열이 아니라 **포지셔닝 선택과 실행 우선순위**로 연결한다.
- “무엇을 이길지”와 동시에 “무엇을 포기할지”를 문서화해 전략 분산을 방지한다.
- 제품/세일즈/운영이 공통으로 참조 가능한 5축 경쟁 분석 표준을 제공한다.

## 2) 원문 핵심 요약 (ccforpms)
- 시장은 PM & Collaboration Software로, TAM 약 $20B 및 고성장 환경에서 async 협업·AI·UX 기대가 핵심 경쟁 변수다.
- Asana는 브랜드·기능 폭·연동 생태계는 강하지만 복잡성과 가격 부담이 약점이다.
- Linear는 속도와 개발자 UX가 강점이나 cross-functional 팀 확장성과 PM 범용 기능에서 제약이 있다.
- Monday.com/ClickUp은 올인원·커스터마이징 강점이 있으나 복잡도 증가와 성능 저하 리스크가 반복된다.
- Notion은 문서+작업 통합 장점이 있으나 전문 PM 워크플로우/대규모 운영에서 한계가 있다.
- TaskFlow 강점은 async-first, 성능, PM+엔지니어 협업 UX이며 약점은 브랜드 인지도, integration 수, enterprise 성숙도다.
- Q1 우선 추진은 SSO/audit/advanced permissions/mobile/integration 확장으로 제시된다.

### ccforpms 단계별 실행 예시 (실전 재현)
1. **경쟁사 모니터링 범위를 먼저 확정한다.**
   - 입력 예시: Asana, Linear, Monday.com, ClickUp, Notion.
   - 실행 액션: 3~5개 핵심 경쟁사와 월간 수집 주기를 고정한다.
   - 기대 출력: 분석 대상이 흔들리지 않는 기준 목록.
2. **릴리스 변화를 5축 템플릿으로 수집한다.**
   - 입력 예시: Product/Pricing/Positioning/Performance/People.
   - 실행 액션: 각 축별 변화 포인트를 한 줄씩 기록한다.
   - 기대 출력: 기능 나열이 아닌 구조화된 비교표.
3. **우리 포지셔닝 필터로 1차 분류한다.**
   - 입력 예시: 속도·맥락·단순성 기준 통과 여부.
   - 실행 액션: 부합/비부합으로 나누고 근거를 남긴다.
   - 기대 출력: 무의미한 추격 기능 사전 차단.
4. **대응안을 즉시/관찰/미추격으로 분류한다.**
   - 입력 예시: "AI 요약 기능은 관찰, SSO 강화는 즉시".
   - 실행 액션: 실행 우선순위와 담당자를 함께 지정한다.
   - 기대 출력: 팀 간 대응 속도와 일관성 향상.
5. **즉시 대응 항목은 Super MVP로 축소한다.**
   - 입력 예시: "2주 내 검증 가능한 최소 기능".
   - 실행 액션: 범위를 줄여 KPI 검증 가능한 형태로 설계한다.
   - 기대 출력: 빠른 실험과 리스크 통제 동시 달성.
6. **미추격 결정도 문서로 남긴다.**
   - 입력 예시: "복잡도 증가 대비 KPI 영향 낮음".
   - 실행 액션: 비추격 사유와 재평가 시점을 기록한다.
   - 기대 출력: feature bloat 방지와 전략 일관성 유지.
7. **주간 리뷰에서 로드맵 반영 여부를 확정한다.**
   - 입력 예시: Activation/Retention 영향으로 채택 판정.
   - 실행 액션: KPI 결과와 비용을 함께 평가해 결정한다.
   - 기대 출력: 경쟁 대응이 실제 제품 성과로 연결됨.

### Claude Code 핵심 기능 연결 (official 반영)
- **경쟁 정보의 실행 문서화**: 경쟁사 변화를 분석 메모로 끝내지 않고 즉시/관찰/미추격 실행안으로 변환한다.
- **Plan 모드 기반 대응 범위 축소**: 대응 아이디어를 Super MVP 수준으로 줄여 2주 내 검증 가능하게 만든다.
- **자연어 기반 포지셔닝 필터 적용**: "속도·맥락·단순성" 기준을 문장으로 명확히 고정해 추격 범위를 제어한다.
- **결정 로그 추적성 강화**: 미추격 사유와 재평가 시점을 남겨 feature bloat를 예방한다.
- **주간 KPI 환류 루프**: 경쟁 대응 결과를 Activation/Retention 등 실제 지표와 연결해 채택 여부를 판정한다.
- **부서 간 공통 메시지 정렬 지원**: 제품·세일즈·운영이 동일 문맥으로 대응 설명을 공유할 수 있다.

## 3) 이든 철학 반영 포인트 (v2.4)
- **P5 한국 실무 적합성**: 글로벌 경쟁 프레임을 그대로 복제하지 않고 국내 팀의 도입 장벽(예산/운영 인력/보안 요구)까지 반영한다.
- **FR-021 5축 표준화**: Product, Pricing, Positioning, Performance, People 5축으로 경쟁 비교를 강제한다.
- **P4 속도-검증 균형**: 경쟁사 대응 아이디어는 즉시 실행하되, KPI/리스크 검증 없는 기능 추격은 금지한다.
- **P8 Connected Hub**: 경쟁 데이터 수집(Claude.ai) → 실행 문서화(Claude Code) → 주간 공유/알림(n8n+Slack)으로 운영한다.
- **P7 의사결정 로그**: “왜 따라가지 않았는지”를 남겨 feature bloat를 방지한다.

## 4) 실무 적용 시나리오
### 시나리오: 경쟁사 AI 기능 출시 대응
1. 경쟁사 릴리스 노트를 5축 표준 템플릿으로 정리한다.
2. 우리 제품의 포지션(속도·맥락·단순성)에 부합하는지 1차 필터링한다.
3. 부합하지 않으면 “미추격 결정”으로 로그를 남기고 모니터링 항목만 유지한다.
4. 부합하면 2주 내 실험 가능한 Super MVP 범위로 축소한다.
5. 실험 결과를 OMTM(Activation/Retention 등)과 연결해 정식 로드맵 편입 여부를 판단한다.

### 이번 주 완료형 실행 플랜
- **월~화 실행**: 핵심 경쟁사 3~5개의 최신 릴리스/가격/포지셔닝 변화를 5축 템플릿으로 수집해 기준 데이터를 갱신한다.
- **수~목 실행**: 대응 후보를 즉시/관찰/미추격으로 분류하고, 즉시 항목은 2주 내 검증 가능한 실험 범위로 축소한다.
- **금요일 검증**: 경쟁 대응안별 KPI 목표와 중단 조건을 점검해 로드맵 반영 여부를 최종 결정한다.
- **Done 기준**: 5축 비교표, 대응 우선순위 보드, 미추격 결정 근거가 최신 상태로 정리되어 제품/세일즈가 동일 전략을 공유한다.
- **리스크/완화**: 기능 추격 압박이 커지면 우리 포지셔닝 기준(속도/맥락/단순성) 1차 필터를 반드시 통과한 항목만 실행하도록 게이트를 둔다.

### 실무 적용 시나리오 (초보자용 상세 실행안)
**대상 상황**: 경쟁사가 AI 기능을 대대적으로 출시해 내부에서 "당장 따라가자" 압력이 높아진 주간.

- **월요일**
  - Owner: PM, 시장 리서처
  - Action: 경쟁사 최신 릴리스/가격/포지셔닝 변화 수집
  - Artifact: 5축 경쟁 비교 초안
  - Check: 핵심 경쟁사 3~5개 데이터 누락 없이 확보
- **화요일**
  - Owner: PM
  - Action: 우리 포지셔닝 필터(속도/맥락/단순성)로 1차 분류
  - Artifact: 부합/비부합 분류표
  - Check: 각 항목에 통과/탈락 근거 한 줄 이상 기록
- **수요일**
  - Owner: PM, 개발 리드
  - Action: 즉시 대응 후보를 Super MVP 범위로 축소하고 KPI 목표 설정
  - Artifact: 대응 실험 브리프, KPI 목표표
  - Check: 2주 내 검증 가능한 범위인지 확인
- **목요일**
  - Owner: PM, 세일즈/CS
  - Action: 즉시/관찰/미추격 결정안을 공유하고 대외 메시지 정렬
  - Artifact: 대응 우선순위 보드, 커뮤니케이션 가이드
  - Check: 제품·세일즈가 동일한 설명으로 고객 대응 가능한지 검증
- **금요일**
  - Owner: PM, 리더십
  - Action: 대응안 최종 승인, 로드맵 반영 또는 보류 결정
  - Artifact: 최종 결정 로그, 차주 실행 계획
  - Check: KPI 기준 및 중단 조건이 문서화되었는지 확인

**리스크 트리거(발동 조건)**
- 수요일까지 즉시 대응 후보의 KPI 목표가 정의되지 않았거나, 포지셔닝 필터 미통과 항목이 포함되면 무분별 추격 위험으로 판단한다.

**Rollback/Mitigation 흐름**
1. 미검증 대응 항목을 즉시 보류하고 관찰 트랙으로 이동한다.
2. 포지셔닝 필터를 재적용해 통과 항목만 남긴다.
3. 남은 항목을 Super MVP로 축소해 2주 실험 계획으로 재작성한다.
4. 금요일 결정은 "즉시 1개 + 관찰 다수" 원칙으로 보수적으로 확정한다.

### 자주 막히는 지점과 해결 가이드
1. **막힘: 경쟁사 기능을 그대로 따라가려다 범위가 과도하게 커짐**
   - 해결: "우리 포지션에 맞는 핵심 가치 1개"만 남기고 나머지는 제외한다.
   - 확인: 실험 범위를 2주 내 배포 가능한 수준으로 줄였는지 점검한다.
2. **막힘: 분석 자료는 많은데 실행 우선순위가 안 정해짐**
   - 해결: 즉시/관찰/미추격 3분류를 강제하고 각 분류에 담당자를 지정한다.
   - 확인: 분류되지 않은 항목은 의사결정 회의 안건에서 제외한다.
3. **막힘: 미추격 결정을 설명 못해 내부 반발 발생**
   - 해결: 비추격 사유를 KPI 영향·복잡도·운영비용 관점으로 문서화한다.
   - 확인: 재평가 시점과 관찰 지표를 함께 남겨 신뢰를 확보한다.
4. **막힘: 세일즈와 제품 메시지가 달라 고객 대응이 혼선**
   - 해결: 목요일에 공통 메시지 가이드를 배포하고 FAQ를 통일한다.
   - 확인: 샘플 고객 질문 3개에 대해 팀이 동일 답변을 하는지 점검한다.

### PM Use Case 확장 (Claude Code 기능 매핑)
| PM 상황 | Claude Code 기능 | 실행 예시 | 검증 포인트 |
|---|---|---|---|
| 경쟁사 신기능 출시로 내부 추격 압박이 큼 | Plan 모드로 대응 범위 축소 | 즉시 대응 1개만 Super MVP로 축소해 실행 | 2주 내 검증 가능 여부 |
| 경쟁 분석이 실행으로 안 이어짐 | 실행 분류 문서화(즉시/관찰/미추격) | 항목별 담당자·기한·KPI를 함께 기록 | 분류 누락 항목 0건 |
| 미추격 결정 설명이 어려움 | 의사결정 로그 표준화 | KPI 영향/복잡도/운영비용 근거를 남김 | 내부 반발 이슈 감소 |
| 제품/세일즈 메시지가 불일치 | 공통 문맥 정렬 | 경쟁 대응 FAQ를 공통 문서로 배포 | 고객 대응 답변 일관성 확보 |
| 대응 후 성과 판단이 모호함 | KPI 환류 기반 판정 | Activation/Retention 변화로 채택·보류 결정 | 로드맵 반영 근거 명확화 |

### Claude Code 핵심개념 체크리스트 (표준 템플릿)
- [ ] 이 문서의 목표를 **문제-행동-성과지표** 1문장으로 적었다.
- [ ] 작업 시작 전에 **모드(Plan / Edit / Auto-Accept)** 선택 기준을 기록했다.
- [ ] `CLAUDE.md`의 프로젝트 규칙(금지사항/품질게이트/보고형식)을 확인했다.
- [ ] `@` 또는 첨부 자료로 **맥락 입력(문서/데이터/정책) 3개 이상** 연결했다.
- [ ] 필요한 경우 **sub-agent/병렬 작업** 범위와 합성(synthesis) 책임자를 지정했다.
- [ ] **MCP/외부도구 연동 필요 여부**와 접근 권한 상태를 점검했다.
- [ ] 산출물에 **검증 기준(정확성/재현성/지표/테스트)** 을 명시했다.
- [ ] 결과를 **Git/문서 링크/로그**로 추적 가능하게 남겼다.
- [ ] 다음 액션 3개를 **담당자·기한·검증포인트**와 함께 확정했다.

## 5) 실습/적용 체크리스트
- [ ] 경쟁사 3~5개를 5축(Product/Pricing/Positioning/Performance/People)으로 비교했다.
- [ ] 각 경쟁사별 우리 강점/약점/즉시 대응/비대응 항목을 분리했다.
- [ ] 대응 아이템마다 성공 지표와 중단 기준을 함께 정의했다.
- [ ] 추격 금지 항목(feature bloat 위험)을 명시했다.
- [ ] 월간 경쟁 리뷰를 운영 캘린더에 고정했다.

## 6) 산출물
- 5축 경쟁 분석 매트릭스(월간 업데이트)
- 대응 우선순위 보드(즉시/관찰/미추격)
- 경쟁 대응 실험 로그(KPI/결정/학습 포인트)
- 미추격 결정 기록부(비추격 사유/재평가 시점/관찰 지표)

### 원문 링크 (Claude Code / Tutorials)
- Claude Code Overview: https://code.claude.com/docs/en/overview
- Tutorials Hub: https://claude.com/resources/tutorials
- Getting Started with Claude AI: https://claude.com/resources/tutorials/getting-started-with-claude-ai
- Claude Cowork: A Research Preview: https://claude.com/resources/tutorials/claude-cowork-a-research-preview
- Get the most from Claude Opus 4.6: https://claude.com/resources/tutorials/get-the-most-from-claude-opus-4-6
